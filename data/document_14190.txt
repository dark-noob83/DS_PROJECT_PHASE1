Part of the Database glossary: Apache Cassandra is an open source distributed database system that is designed for storing and managing large amounts of data across commodity servers. 
Apache Cassandra, a top level Apache project born at Facebook and built on Amazon’s Dynamo and Google’s BigTable, is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure. 
Contrasting Hadoop & Apache Cassandra. Apache Cassandra is a NoSQL database ideal for high-speed, online transactional data, while Hadoop is a big data analytics system that focuses on data warehousing and data lake use cases. HBase is sometimes used for an online application because an existing Hadoop implementation exists at a site and not because it is the right fit for the application. HBase is typically not a good choice for developing always-on online applications and is nearly 2-3 years behind Cassandra in many technical respects.
Hadoop is a leading solution for nonrelational data storage and processing. It is based on the key Google technologies for storing and processing huge data sets distributed across large clusters of commodity computers. Hadoop has shown rapid adoption in the enterprise, and is undoubtedly the leading technology in big data data processing. The foundation of the Hadoop system is the Hadoop Distributed Filesystem (HDFS) which can store massive distributed unstructured data sets.
Hadoop is not a type of database, but rather a software ecosystem that allows for massively parallel computing. It is an enabler of certain types NoSQL distributed databases (such as HBase), which can allow for data to be spread across thousands of servers with little reduction in performance. 
Apache Hadoop is an open-source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. The list includes the HBase database, the Apache Mahout machine learning system, and the Apache Hive Data Warehouse system. Hadoop can in theory be used for any sort of work that is batch-oriented rather than real-time, is very data-intensive, and benefits from parallel processing of data.
Helenos is a graphical user interface for Cassandra. Apache Cassandra is an open source distributed database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure. Cassandra's data model is a partitioned row store with tunable consistency. Rows are organized into tables; the first component of a table's primary key is the partition key; within a partition, rows are clustered by the remaining columns of the key.
Brisk uses the Cassandra database to replace Hadoop's HDFS filesystem and the HBase database. Both HBase and Cassandra can deal with large data sets, and provide high transaction rates and low latency lookups. Both allow map-reduce processing to be run against the database when aggregation or parallel processing is required. Hadoop has shown rapid adoption in the enterprise, and is undoubtedly the leading technology in big data data processing. The foundation of the Hadoop system is the Hadoop Distributed Filesystem (HDFS) which can store massive distributed unstructured data sets.
HBase is an open source, NoSQL, distributed database modeled after Google's BigTable and is written in Java. It’s included as part of Apache Software Foundation's Apache Hadoop project and runs on top of HDFS (Hadoop Distributed File System), providing BigTable-like capabilities for Hadoop. HBase is sometimes used for an online application because an existing Hadoop implementation exists at a site and not because it is the right fit for the application. HBase is typically not a good choice for developing always-on online applications and is nearly 2-3 years behind Cassandra in many technical respects.
The Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing. The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. 1 Hadoop Common: The common utilities that support the other Hadoop modules. 2  Hadoop Distributed File System (HDFS™) : A distributed file system that provides high-throughput access to application data. 3  Hadoop YARN: A framework for job scheduling and cluster resource management.