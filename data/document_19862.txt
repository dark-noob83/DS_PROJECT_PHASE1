The mutation rate of an organism is an evolved characteristic and is strongly influenced by the genetics of each organism, in addition to strong influence from the environment. The upper and lower limits to which mutation rates can evolve is the subject of ongoing investigation. The mutation spectrum of an organism is the rate at which different mutations occur at different sites. Typically two sites are considered, each of which may have three mutations, resulting in six total rates for most mutation spectra. The two sites are the two correct pairs possible in DNA: A:T pairs and C:G pairs;
If this mutation rate is accurate, then the average human-chimpanzee gene divergence has to be up around 11 million years ago. That can be accommodated with a 7-million-year-old species divergence only if we assume a very large ancestral population -- on the order of 50,000 or higher. A divergence date of 6 million years gave rise to a human per-site mutation rate of 0.65 x 10 -9 per year (1.3 x 10 -8 per 20-year generation). A divergence date of 7 million years lowered the mutation rate to 0.57 x 10 -9 per year. Low mutation rates do not always result from these studies.
Eighteen processed pseudogenes were sequenced, including 12 on autosomes and 6 on the X chromosome. The average mutation rate was estimated to be approximately 2.5 x 10(-8) mutations per nucleotide site or 175 mutations per diploid genome per generation. 
From Wikipedia, the free encyclopedia. In genetics, the mutation rate is a measure of the rate at which various types of mutations occur over time. Mutation rates are typically given for a specific class of mutation, for instance point mutations, small or large scale insertions or deletions. The mutation spectrum of an organism is the rate at which different mutations occur at different sites. Typically two sites are considered, each of which may have three mutations, resulting in six total rates for most mutation spectra. The two sites are the two correct pairs possible in DNA: A:T pairs and C:G pairs;
In genetics, the mutation rate is a measure of the rate at which various types of mutations occur during some unit of time. Mutation rates are typically given for a specific class of mutation, for instance point mutations, small or large scale insertions or deletions. 
In fact, we expect that across all humans, there are over 100 mutations at every single position in the human genome. And, here is the math to prove it: The average mutation rate in the human genome is 1.2 x10^-8 mutations per site per generation. This is pretty small, so in each person, we only expect to observe a handful of new mutations relative to their parents. But, that handful of mutations adds up when you think of how many people are on the earth. 
Previous work had suggested a human mutation rate around 2.5 x 10 -8 per site per generation. The new study found less than half the expected number of mutations between these parents and offspring, an estimated rate of only 1.1 x 10 -8 per site. A divergence date of 6 million years gave rise to a human per-site mutation rate of 0.65 x 10 -9 per year (1.3 x 10 -8 per 20-year generation). A divergence date of 7 million years lowered the mutation rate to 0.57 x 10 -9 per year. Low mutation rates do not always result from these studies.
Mutation rates were calculated for a range of different human-chimpanzee divergence times and for two different ancestral population sizes. Mutation rate estimates vary from 1.3 x 10-8 (assuming T = 6 mya and Ne = 105) to 2.7 x 10-8 (assuming T = 4.5 mya and Ne = 104). A divergence date of 6 million years gave rise to a human per-site mutation rate of 0.65 x 10 -9 per year (1.3 x 10 -8 per 20-year generation). A divergence date of 7 million years lowered the mutation rate to 0.57 x 10 -9 per year. Low mutation rates do not always result from these studies.